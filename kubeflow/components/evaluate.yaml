name: Eval
description: |
  Evaluate
inputs:
  - {name: image, description: '', default: ''}
  - {name: s3_bucket, description: '', default: ''}
  - {name: owner, description: '', default: ''}
  - {name: project, description: '', default: ''}
  - {name: experiment, description: '', default: ''}
  - {name: model, description: '', default: ''}
  - {name: model_owner, description: '', default: ''}
  - {name: eval_set, description: '', default: ''}
  - {name: eval_version, description: '', default: ''}
  - {name: additional_args, description: '', default: ''}
outputs:
  - {name: MLPipeline UI metadata, type: UI metadata}
  - {name: MLPipeline Metrics, type: Metrics}
implementation:
  container:
    image: '{{inputs.parameters.image}}'
    command:
    - /bin/bash
    - -ex
    - -c 
    - |
      . /opt/genie-toolkit/lib.sh
      parse_args "$0" "image s3_bucket owner project experiment model model_owner eval_set eval_version" "$@"
      shift $n
      pwd
      aws s3 sync s3://${s3_bucket}/${owner}/workdir/${project} .
      ls -al 
      mkdir -p tmp
      export GENIE_TOKENIZER_ADDRESS=tokenizer.default.svc.cluster.local:8888
      export TZ=America/Los_Angeles
      make geniedir=/opt/genie-toolkit thingpedia_cli=thingpedia experiment=$experiment eval_set=${eval_set} eval_version=${eval_version} model=${model_owner}/${model} evaluate
      make geniedir=/opt/genie-toolkit thingpedia_cli=thingpedia experiment=$experiment eval_set=${eval_set} eval_version=${eval_version} model=${model_owner}/${model} evaluate-upload
      # write ui outputs
      RESULTS_FILE="eval/${experiment}/${eval_set}/${model_owner}/model.nlu.results"
      python3 - ${RESULTS_FILE} << EOF
      import json
      import sys
      results = open(sys.argv[1]).read()
      metadata = {"outputs" : [
        {
          "storage": "inline",
          "source": results,
          "format": 'csv',
          "type": "table",
          "header": ["Eval Set", "Device", "Turn Number", "# turns","Accuracy", 
                     "W/o params", "Function", "Device", "Num Function", "Syntax"]
        }
      ]} 
      with open('/tmp/mlpipeline-ui-metadata.json', 'w') as f:
        json.dump(metadata, f)
      EOF
      # write metrics output
      python3 - ${RESULTS_FILE} << EOF
      import json
      import sys
      import csv
      metrics = []
      sum = 0
      cnt = 0
      for row in csv.reader(open(sys.argv[1])):
        sum += float(row[4])
        cnt += 1
      accuracy = sum/cnt
      metrics = {
        'metrics': [{
          'name': 'average-accuracy',
          'numberValue':  accuracy, 
          'format': "PERCENTAGE",
        }]
      }       
      with open('/tmp/mlpipeline-metrics.json', 'w') as f:
        json.dump(metrics, f)
      EOF

    args: [
      'cmd',
      --image, {inputValue: image},
      --s3_bucket, {inputValue: s3_bucket},
      --owner, {inputValue: owner},
      --project, {inputValue: project},
      --experiment, {inputValue: experiment},
      --model, {inputValue: model},
      --model_owner, {inputValue: model_owner},      
      --eval_set, {inputValue: eval_set},
      --eval_version, {inputValue: eval_version},
      --, {inputValue: additional_args}
    ]
    
    fileOutputs:
      MLPipeline UI metadata: /tmp/mlpipeline-ui-metadata.json
      MLPipeline Metrics: /tmp/mlpipeline-metrics.json
