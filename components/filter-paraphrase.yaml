name: Filter Paraphrases
description: |
  Filter paraphrases using a parser
inputs:
  - {name: image, description: '', default: ''}
  - {name: s3_bucket, description: '', default: ''}
  - {name: owner, description: '', default: ''}
  - {name: task_name, description: '', default: '' }
  - {name: project, description: '', default: ''}
  - {name: experiment, description: '', default: ''}
  - {name: dataset, description: '', default: ''}
  - {name: s3_database_dir, description: '', default: '' }
  - {name: s3_bootleg_prepped_data, description: '', default: ''}
  - {name: s3_input_datadir, description: '', default: ''}
  - {name: filtering_model, description: '', default: ''}
  - {name: filtering_batch_size, description: '', default: ''}
  - {name: paraphrase_subfolder, description: '', default: ''}
  - {name: additional_args, description: '', default: ''}
outputs:
  - {name: s3_output_datadir}
implementation:
  container:
    image: '{{inputs.parameters.image}}'
    command:
    - /bin/bash
    - -c
    - |
      . /opt/genie-toolkit/lib.sh

      parse_args "$0" "image s3_bucket owner task_name project experiment dataset s3_database_dir s3_bootleg_prepped_data s3_input_datadir s3_output_datadir filtering_model filtering_batch_size task_name paraphrase_subfolder" "$@"
      shift $n
      
      set -e
      set -x
      
      /opt/genie-toolkit/sync-repos.sh

      if test "${s3_database_dir}" != None ; then
        aws s3 sync --no-progress ${s3_database_dir}/train/ ./database/
      fi

      if test "${s3_bootleg_prepped_data}" != None ; then
        aws s3 sync --no-progress ${s3_bootleg_prepped_data} results_temp --exclude '*' --include '*bootleg_labels.jsonl'
      fi

      aws s3 sync --no-progress  --exclude "synthetic*.txt" --exclude "*bootleg*" --exclude "*chunked*" "${s3_input_datadir}" input_dataset/
      aws s3 sync --no-progress --exclude '*/dataset/*' --exclude '*/cache/*' --exclude 'iteration_*.pth' --exclude '*_optim.pth' "${filtering_model}" filtering_model/

      mkdir -p output_dataset
      cp -r input_dataset/* output_dataset
      
      s3_output_dir="s3://${s3_bucket}/${owner}/dataset/${project}/${experiment}/${dataset}-gen-filt/`date +%s`/"

      mkdir -p `dirname $s3_output_datadir`
      echo ${s3_output_dir} > $s3_output_datadir

      filtering_arguments="$@"

      case "$task_name" in
        "almond_dialogue_nlu" | "almond_dialogue_nlu_agent")
          is_dialogue=true
          input_dir="input_dataset/$paraphrase_subfolder"
          output_dir="output_dataset/$paraphrase_subfolder"
          filtering_dir="almond/$paraphrase_subfolder"
          ;;

        "almond")
          input_dir="input_dataset"
          output_dir="output_dataset"
          filtering_dir="almond"
          ;;

        *)
          echo "Error: invalid task name."
          exit 1
          ;;
      esac

      run_parser() {
        # get parser output for paraphrased utterances
        mkdir -p filtering_dataset/${filtering_dir}
        cp ${output_dir}/unfiltered.tsv filtering_dataset/${filtering_dir}/unfiltered.tsv
        genienlp predict \
          --data ./filtering_dataset \
          --path filtering_model \
          --eval_dir ./eval_dir \
          --evaluate valid \
          --pred_set_name unfiltered \
          --task ${task_name} \
          --overwrite \
          --silent \
          --main_metric_only \
          --skip_cache \
          --val_batch_size ${filtering_batch_size} \
          $filtering_arguments
        cp ./eval_dir/valid/${task_name}.results.json ${output_dir}/
        # cp ./eval_dir/valid/${task_name}.tsv ${output_dir}/ # useful for debugging the paraphraser
      }

      filter() {
        # remove paraphrases that do not preserve the meaning according to the parser
        mkdir -p results_temp/filtered_bootleg/bootleg_wiki/
        genienlp transform-dataset \
          ${output_dir}/unfiltered.tsv \
          ${output_dir}/filtered.tsv \
          --bootleg_file results_temp/unfiltered_bootleg/bootleg_wiki/bootleg_labels.jsonl \
          --bootleg_output_file results_temp/filtered_bootleg/bootleg_wiki/bootleg_labels.jsonl \
          --thrown_away ${output_dir}/thrown_away.tsv \
          --thingtalk_gold_file eval_dir/valid/${task_name}.tsv \
          --transformation remove_wrong_thingtalk \
          --task ${task_name}
      }

      append_to_original() {
        # append paraphrases to the end of the original training file and remove duplicates
        cp ${input_dir}/train.tsv ${output_dir}/train.tsv
        cp ${output_dir}/filtered.tsv ${output_dir}/temp.tsv
        mkdir -p results_temp/temp_bootleg/bootleg_wiki/
        genienlp transform-dataset \
          ${output_dir}/temp.tsv \
          ${output_dir}/filtered.tsv \
          --bootleg_file results_temp/filtered_bootleg/bootleg_wiki/bootleg_labels.jsonl \
          --bootleg_output_file results_temp/temp_bootleg/bootleg_wiki/bootleg_labels.jsonl \
          --remove_duplicates \
          --task ${task_name}
        rm ${output_dir}/temp.tsv
        cat ${output_dir}/filtered.tsv >> ${output_dir}/train.tsv
        cat results_temp/temp_bootleg/bootleg_wiki/bootleg_labels.jsonl >> results_temp/train_bootleg/bootleg_wiki/bootleg_labels.jsonl
        if test `cat ${output_dir}/train.tsv | wc -l` != `cat results_temp/train_bootleg/bootleg_wiki/bootleg_labels.jsonl | wc -l` ; then
          echo 'Error: number of examples in final train and bootleg files should be the same' ; exit 1
        fi
      }

     
      if ! test -f ${input_dir}/unfiltered.tsv ; then
        echo "Error: unfiltered.tsv does not exist."
        exit 1
      fi

      run_parser
      filter
      append_to_original

      rm ${output_dir}/unfiltered.tsv
      rm ${output_dir}/almond_paraphrase.results.json
      
      # upload the new dataset to S3
      aws s3 sync --no-progress output_dataset/ "${s3_output_dir}"
    
    args: [
      'cmd',
      --image, {inputValue: image},
      --s3_bucket, {inputValue: s3_bucket},
      --owner, {inputValue: owner},
      --task_name, {inputValue: task_name},
      --project, {inputValue: project},
      --experiment, {inputValue: experiment},
      --dataset, {inputValue: dataset},
      --s3_database_dir, { inputValue: s3_database_dir },
      --s3_bootleg_prepped_data, {inputValue: s3_bootleg_prepped_data},
      --s3_input_datadir, {inputValue: s3_input_datadir},
      --s3_output_datadir, {outputPath: s3_output_datadir},
      --filtering_model, {inputValue: filtering_model},
      --filtering_batch_size, {inputValue: filtering_batch_size},
      --paraphrase_subfolder, {inputValue: paraphrase_subfolder},
      --, {inputValue: additional_args}, 
    ]
    
