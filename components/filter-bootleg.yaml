name: Filter Bootleg
description: |
  Filter train/dev set given Bootleg output
inputs:
  - {name: image, description: ''}
  - {name: s3_bucket, description: ''}
  - {name: owner, description: ''}
  - {name: task_name, description: ''}
  - {name: project, description: ''}
  - {name: experiment, description: ''}
  - {name: dataset, description: ''}
  - {name: valid_set, description: ''}
  - {name: s3_datadir, description: ''}
  - {name: s3_bootleg_prepped_data, description: ''}
  - {name: s3_bootleg_subfolder, description: ''}
  - {name: dataset_subfolder, description: ''}
  - {name: bootleg_model, description: ''}
  - {name: additional_args, description: ''}
outputs:
  - {name: s3_output_datadir}
  - {name: s3_output_bootlegdir}
implementation:
  container:
    image: '{{inputs.parameters.image}}'
    command:
    - /bin/bash
    - -ex
    - -c
    - |
      . /opt/genie-toolkit/lib.sh

      parse_args "$0" "image s3_bucket owner task_name project experiment valid_set dataset s3_datadir s3_bootleg_prepped_data s3_bootleg_subfolder dataset_subfolder bootleg_model s3_output_datadir s3_output_bootlegdir" "$@"
      shift $n
      cd $HOME

      /opt/genie-toolkit/sync-repos.sh
      MAKEDIR=`get_make_dir ${project}`
      cat >> workdir/config.mk <<EOF
      developer_key = ${THINGPEDIA_DEVELOPER_KEY}
      EOF
      cd workdir/${MAKEDIR}

      set +x
      keyctl session ; export AZCOPY_SPA_CLIENT_SECRET=${AZURE_SP_PASSWORD} ; azcopy login --service-principal --application-id ${AZURE_SP_APP_ID} --tenant-id ${AZURE_SP_TENANT_ID}
      set -x
      
      mkdir -p ./datadir
      mkdir -p ./bootlegdir
      azcopy sync --recursive ${s3_bucket%/}/${s3_datadir%/} ./datadir/ --exclude-path "chunked/"
      azcopy sync --recursive ${s3_bucket%/}/${s3_bootleg_prepped_data} ./bootlegdir/

      export TZ=America/Los_Angeles
      make "-j${parallel}" \
          geniedir=/opt/genie-toolkit \
          "owner=${owner}" \
          "genie_k8s_owner=${owner}" \
          "genie_k8s_project=${project}" \
          "s3_bucket=${s3_bucket}" \
          thingpedia_cli=thingpedia \
          "experiment=${experiment}" \
          "valid_set=${valid_set}"\
          $@ \
          datadir-postprocessed

      hash=`date +%s`
      S3_DATADIR=${s3_bucket%/}/${owner}/dataset/${project}/${experiment}/${dataset}/${hash}/
      azcopy sync --recursive datadir-postprocessed/ ${S3_DATADIR}

      S3_BOOTLEG_PREPPED_DATA=${S3_DATADIR%/}/bootleg/${bootleg_model}/prepped_files/
      azcopy sync --recursive bootlegdir-postprocessed/ ${S3_BOOTLEG_PREPPED_DATA}

      mkdir -p `dirname $s3_output_datadir`
      echo ${S3_DATADIR} > $s3_output_datadir
      mkdir -p `dirname $s3_output_bootlegdir`
      echo ${S3_BOOTLEG_PREPPED_DATA} > $s3_output_bootlegdir

    args: [
      'cmd',
      --image, {inputValue: image},
      --s3_bucket, {inputValue: s3_bucket},
      --owner, {inputValue: owner},
      --task_name, {inputValue: task_name},
      --project, {inputValue: project},
      --experiment, {inputValue: experiment},
      --valid_set, {inputValue: valid_set},
      --dataset, {inputValue: dataset},
      --s3_datadir, {inputValue: s3_datadir},
      --s3_bootleg_prepped_data, {inputValue: s3_bootleg_prepped_data},
      --s3_bootleg_subfolder, {inputValue: s3_bootleg_subfolder},
      --dataset_subfolder, {inputValue: dataset_subfolder},
      --bootleg_model, {inputValue: bootleg_model},
      --s3_output_datadir, {outputPath: s3_output_datadir},
      --s3_output_bootlegdir, {outputPath: s3_output_bootlegdir},
      --, {inputValue: additional_args}
    ]
