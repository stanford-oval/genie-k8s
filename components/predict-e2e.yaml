name: Predict E2E
description: |
  End-to-End Prediction for Dialogues
inputs:
  - {name: image, description: '', default: ''}
  - {name: owner, description: '', default: ''}
  - {name: eval_sets, description: '', default: ''}
  - {name: task_name, description: '', default: ''}
  - {name: model_name_or_path, description: '', default: ''}
  - {name: s3_bucket, description: '', default: ''}
  - {name: s3_input_datadir, description: '', default: ''}
  - {name: model_type, description: '', default: ''}
  - {name: eval_lang, description: '', default: ''}
  - {name: dataset_subfolder, description: '', default: ''}
  - {name: additional_args, description: '', default: ''}
outputs:
  - {name: metrics_output}
  - {name: s3_metrics_output}
  - {name: MLPipeline UI metadata, type: UI metadata}
  - {name: MLPipeline Metrics, type: Metrics}
implementation:
  container:
    image: '{{inputs.parameters.image}}'
    command:
    - /bin/bash
    - -ex
    - -c
    - |
      . /opt/genie-toolkit/lib.sh
      parse_args "$0" "image owner eval_sets task_name model_name_or_path s3_bucket s3_input_datadir model_type eval_lang dataset_subfolder metrics_output s3_metrics_output" "$@"
      shift $n
      cd $HOME

      git clone git@github.com:stanford-oval/genie-workdirs.git

      /opt/genie-toolkit/sync-repos.sh

      modeldir="$HOME/model/"
      mkdir -p "$modeldir"

      if test "${dataset_subfolder}" = "None" ; then
        dataset_subfolder=
      fi

      dataset_dir=dataset

      if [[ "${task_name}" == *"bitod"*  ]] ; then
        test_folder=BiToD
      elif [[ "${task_name}" == *"risawoz"*  ]] ; then
        test_folder=RiSAWOZ
      fi

      set +x
      keyctl session ; export AZCOPY_SPA_CLIENT_SECRET=${AZURE_SP_PASSWORD} ; azcopy login --service-principal --application-id ${AZURE_SP_APP_ID} --tenant-id ${AZURE_SP_TENANT_ID}
      set -x

      mkdir -p $dataset_dir
      azcopy sync --recursive ${s3_bucket%/}/${s3_input_datadir%/}/${dataset_subfolder} $dataset_dir --exclude-pattern "synthetic*.txt" --exclude-path "bootleg/;chunked/"

      # if not an s3 path assume pretrained model name and save it in genienlp format
      if [[ "${model_type}" != None ]] ; then
        genienlp train --train_tasks ${task_name} --train_iterations 0 --save "$modeldir" --model ${model_type} --pretrained_model ${model_name_or_path} --exist_ok
        s3_output=${s3_bucket%/}/${owner}/workdir/prediction/${task_name}/${model_type}/${model_name_or_path}
      else
        if [[ "$@" == *"--checkpoint_name"* ]] ; then
          azcopy sync --recursive "${s3_bucket%/}/${model_name_or_path}" "$modeldir"/ --exclude-pattern "*_optim.pth;*.log" --exclude-path "eval/;pred/"
        else
          azcopy sync --recursive "${s3_bucket%/}/${model_name_or_path}" "$modeldir"/ --exclude-pattern "iteration_*.pth;*_optim.pth;*.log" --exclude-path "eval/;pred/"
        fi
        s3_output=${s3_bucket%/}/${model_name_or_path%/}
      fi

      S3_METRIC_OUTPUT="$s3_output"/pred_e2e/`date +%s`/

      mkdir -p `dirname $s3_metrics_output`
      mkdir -p $metrics_output

      echo $S3_METRIC_OUTPUT >> $s3_metrics_output

      for e_set in ${eval_sets} ; do
        genienlp predict \
            --data "dataset" \
            --path "$modeldir" \
            --eval_dir "./eval" \
            --evaluate valid \
            --pred_set_name ${e_set} \
            --task ${task_name} \
            --overwrite \
            --silent \
            --e2e_dialogue_evaluation \
            $@

        python3 genie-workdirs/zeroshot/scripts/compute_e2e.py \
          --reference_file_path genie-workdirs/zeroshot/${test_folder}/data/"$eval_lang"_"$e_set".json \
          --prediction_file_path ./eval/valid/e2e_dialogue_preds.json \
          --experiment ${task_name} \
          --setting ${eval_lang}

        # rename valid subfolder; if e_set is valid move it to a temp directory
        if test "${e_set}" != "valid" ; then
          mv ./eval/valid/ ./eval/$e_set/
        else
          mv ./eval/valid/ ./eval/temp/
        fi

      done

      # move temp directory back to valid
      if [ -d ./eval/temp/ ] ; then
        mv ./eval/temp/ ./eval/valid/
      fi

      cp -r ./eval/* $metrics_output/

      genienlp write-kf-metrics --eval_results `for e_set in ${eval_sets} ; do echo ./eval/$e_set/e2e_dialogue_results.json ; done`

      azcopy sync --recursive "./eval" $S3_METRIC_OUTPUT

    args: [
      'cmd',
      --image, {inputValue: image},
      --owner, {inputValue: owner},
      --eval_sets, {inputValue: eval_sets},
      --task_name, {inputValue: task_name},
      --model_name_or_path, {inputValue: model_name_or_path},
      --s3_bucket, {inputValue: s3_bucket},
      --s3_input_datadir, {inputValue: s3_input_datadir},
      --model_type, {inputValue: model_type},
      --eval_lang, {inputValue: eval_lang},
      --dataset_subfolder, {inputValue: dataset_subfolder},
      --s3_metrics_output, {outputPath: s3_metrics_output},
      --metrics_output, {outputPath: metrics_output},
      --, {inputValue: additional_args}
    ]

    fileOutputs:
      MLPipeline UI metadata: /tmp/mlpipeline-ui-metadata.json
      MLPipeline Metrics: /tmp/mlpipeline-metrics.json
