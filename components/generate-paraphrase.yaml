name: Generate Paraphrases
description: |
  Automatic paraphrasing of a dataset (without filtering)
inputs:
  - {name: image, description: '', default: ''}
  - {name: s3_bucket, description: '', default: ''}
  - {name: owner, description: '', default: ''}
  - {name: task_name, description: '', default: '' }
  - {name: project, description: '', default: ''}
  - {name: experiment, description: '', default: ''}
  - {name: dataset, description: '', default: ''}
  - {name: s3_input_datadir, description: '', default: ''}
  - {name: paraphrasing_model, description: '', default: ''}
  - {name: keep_original_duplicates, description: '', default: ''}
  - {name: paraphrase_subfolder, description: '', default: ''}
  - {name: additional_args, description: '', default: ''}
outputs:
  - {name: s3_output_datadir}
implementation:
  container:
    image: '{{inputs.parameters.image}}'
    command:
    - /bin/bash
    - -c
    - |
      . /opt/genie-toolkit/lib.sh

      parse_args "$0" "image s3_bucket owner task_name project experiment dataset s3_input_datadir s3_output_datadir paraphrasing_model keep_original_duplicates task_name paraphrase_subfolder" "$@"
      shift $n

      set -e
      set -x

      /opt/genie-toolkit/sync-repos.sh

      mkdir -p input_dataset/
      rsync -rv --exclude "synthetic*.txt" --exclude "*bootleg*" --exclude "*chunked*" ${s3_input_datadir}* input_dataset/
      mkdir -p paraphraser/
      rsync -rv --exclude '*checkpoint* ${paraphrasing_model}* paraphraser/

      mkdir -p output_dataset
      cp -r input_dataset/* output_dataset

      s3_output_dir="/shared/data/${owner}/dataset/${project}/${experiment}/${dataset}-gen/`date +%s`/"

      mkdir -p `dirname $s3_output_datadir`
      echo ${s3_output_dir} > $s3_output_datadir

      paraphrasing_arguments="$@"

      case "$task_name" in
        "almond_dialogue_nlu" | "almond_dialogue_nlu_agent")
          is_dialogue=true
          input_dir="input_dataset/$paraphrase_subfolder"
          output_dir="output_dataset/$paraphrase_subfolder"
          ;;

        "almond")
          input_dir="input_dataset"
          output_dir="output_dataset"
          ;;

        *)
          echo "Error: invalid task name."
          exit 1
          ;;
      esac

      make_input_ready() {
          if [ "$keep_original_duplicates" = false ] ; then
            # remove duplicates before paraphrasing to avoid wasting effort
            genienlp transform-dataset \
              ${input_dir}/train.tsv \
              ${input_dir}/train_no_duplicate.tsv \
              --remove_duplicates \
              --task ${task_name}
              cp ${input_dir}/train_no_duplicate.tsv ${input_dir}/train.tsv
          fi
      }

      run_paraphrase() {
        echo $paraphrasing_arguments

        mkdir -p ${input_dir}/almond/
        cp ${input_dir}/train.tsv ${input_dir}/almond/train.tsv

        # run paraphrase generation
        genienlp predict \
        --task almond_paraphrase \
        --path paraphraser \
        --data ${input_dir} \
        --eval_dir eval_dir \
        --evaluate train \
        --overwrite \
        --silent \
        $paraphrasing_arguments

        mv eval_dir/train/* ${output_dir}/
      }

      join() {
        # join the original file and the paraphrasing output
        genienlp transform-dataset \
          ${input_dir}/train.tsv \
          ${output_dir}/unfiltered.tsv \
          --query_file ${output_dir}/almond_paraphrase.tsv \
          --transformation replace_queries \
          --remove_with_heuristics \
          --task ${task_name}
      }

      make_input_ready
      run_paraphrase
      join

      # Delete intermediate files that don't help debugging
      rm ${output_dir}/almond_paraphrase.tsv

      # upload the new dataset to S3
      mkdir -p ${s3_output_dir}
      rsync -rv output_dataset/ ${s3_output_dir}

    args: [
      'cmd',
      --image, {inputValue: image},
      --s3_bucket, {inputValue: s3_bucket},
      --owner, {inputValue: owner},
      --task_name, {inputValue: task_name},
      --project, {inputValue: project},
      --experiment, {inputValue: experiment},
      --dataset, {inputValue: dataset},
      --s3_input_datadir, {inputValue: s3_input_datadir},
      --s3_output_datadir, {outputPath: s3_output_datadir},
      --paraphrasing_model, {inputValue: paraphrasing_model},
      --keep_original_duplicates, {inputValue: keep_original_duplicates},
      --paraphrase_subfolder, {inputValue: paraphrase_subfolder},
      --, {inputValue: additional_args},
    ]
