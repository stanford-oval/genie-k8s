name: Test PF
description: |
  Test PF
inputs:
  - {name: image, description: '', default: ''}
  - {name: owner, description: '', default: ''}
  - {name: eval_sets, description: '', default: ''}
  - {name: task_name, description: '', default: ''}
  - {name: model_name_or_path, description: '', default: ''}
  - {name: s3_input_datadir, description: '', default: ''}
  - {name: s3_database_dir, description: '', default: ''}
  - {name: pf_version, description: '', default: ''}
  - {name: model_type, description: '', default: ''}
  - {name: dataset_subfolder, description: '', default: ''}
  - {name: val_batch_size, description: '', default: ''}
  - {name: additional_args, description: '', default: ''}
implementation:
  container:
    image: '{{inputs.parameters.image}}'
    command:
    - /bin/bash
    - -ex
    - -c
    - |
      . /opt/genie-toolkit/lib.sh
      parse_args "$0" "image owner eval_sets task_name model_name_or_path s3_input_datadir s3_database_dir pf_version model_type dataset_subfolder val_batch_size" "$@"
      shift $n
      cd $HOME

      /opt/genie-toolkit/sync-repos.sh

      modeldir="$HOME/model/"
      mkdir -p "$modeldir"

      if test "${dataset_subfolder}" = "None" ; then
        dataset_subfolder=
      fi

      case $task_name in
        "almond"*)
          dataset_dir=dataset/almond
        ;;

        *)
           dataset_dir=dataset
        ;;
      esac

      aws s3 sync --no-progress ${s3_input_datadir}${dataset_subfolder} $dataset_dir --exclude "synthetic*.txt" --exclude "*bootleg*" --exclude "*chunked*"

      if test "${s3_database_dir}" != None ; then
        aws s3 sync --no-progress ${s3_database_dir}/bootleg/ ./database/
      fi

      # if not an s3 path assume pretrained model name and save it in genienlp format
      if [[ "${model_name_or_path}" != "s3://"* ]] ; then
        genienlp train --train_tasks ${task_name} --train_iterations 0 --save "$modeldir" --model ${model_type} --pretrained_model ${model_name_or_path} --exist_ok
        s3_output=s3://geniehai/${owner}/workdir/prediction/${task_name}/${model_type}/${model_name_or_path}
      else
        if [[ "$@" == *"--checkpoint_name"* ]] ; then
          aws s3 sync --no-progress "${model_name_or_path}" "$modeldir"/ --exclude "*_optim.pth" --exclude "*eval/*" --exclude '*pred/*' --exclude "*.log"
        else
          aws s3 sync --no-progress "${model_name_or_path}" "$modeldir"/ --exclude "iteration_*.pth" --exclude "*_optim.pth" --exclude "*eval/*" --exclude '*pred/*' --exclude "*.log"
        fi
        s3_output=${model_name_or_path%/}
      fi

      for e_set in ${eval_sets} ; do
        python3 /opt/genienlp/genienlp/test_pf.py \
            --data "dataset" \
            --path "$modeldir" \
            --eval_dir "./eval" \
            --evaluate valid \
            --pred_set_name ${e_set} \
            --task ${task_name} \
            --overwrite \
            --silent \
            --val_batch_size ${val_batch_size} \
            $@
      done


    args: [
      'cmd',
      --image, {inputValue: image},
      --owner, {inputValue: owner},
      --eval_sets, {inputValue: eval_sets},
      --task_name, {inputValue: task_name},
      --model_name_or_path, {inputValue: model_name_or_path},
      --s3_input_datadir, {inputValue: s3_input_datadir},
      --s3_database_dir, {inputValue: s3_database_dir},
      --pf_version, {inputValue: pf_version},
      --model_type, {inputValue: model_type},
      --dataset_subfolder, {inputValue: dataset_subfolder},
      --val_batch_size, {inputValue: val_batch_size},
      --, {inputValue: additional_args}
    ]
