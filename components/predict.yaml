name: Predict
description: |
  Predict
inputs:
  - {name: image, description: '', default: ''}
  - {name: eval_set, description: '', default: ''}
  - {name: task_name, description: '', default: ''}
  - {name: s3_model_dir, description: '', default: ''}
  - {name: s3_input_datadir, description: '', default: ''}
  - {name: dataset_subfolder, description: '', default: ''}
  - {name: val_batch_size, description: '', default: ''}
  - {name: additional_args, description: '', default: ''}
outputs:
  - {name: s3_output_datadir}
implementation:
  container:
    image: '{{inputs.parameters.image}}'
    command:
    - /bin/bash
    - -ex
    - -c 
    - |
      . /opt/genie-toolkit/lib.sh
      parse_args "$0" "image eval_set task_name s3_model_dir s3_input_datadir dataset_subfolder s3_output_datadir val_batch_size" "$@"
      shift $n
      cd $HOME

      /opt/genie-toolkit/sync-repos.sh
      
      modeldir="$HOME/model/"
      mkdir -p "$modeldir"

      if test "${dataset_subfolder}" = "None" ; then
        dataset_subfolder=
      fi

      aws s3 sync --no-progress "${s3_model_dir}" "$modeldir"/ --exclude "iteration_*.pth" --exclude "*_optim.pth" --exclude "*eval/*"  --exclude "*.log"
      aws s3 sync --no-progress --exclude "synthetic*.txt" --exclude "*bootleg*" ${s3_input_datadir}${dataset_subfolder} dataset/almond

      genienlp predict \
          --data "dataset" \
          --path "$modeldir" \
          --eval_dir "./eval" \
          --evaluate "$eval_set" \
          --task ${task_name} \
          --overwrite \
          --silent \
          --skip_cache \
          --val_batch_size ${val_batch_size} \
          $@

      S3_OUTPUT_DATADIR=${s3_model_dir%/}/eval/`date +%s`
      aws s3 sync --no-progress "./eval" ${S3_OUTPUT_DATADIR}

      mkdir -p `dirname $s3_output_datadir`
      echo ${S3_OUTPUT_DATADIR} > $s3_output_datadir

    args: [
      'cmd',
      --image, {inputValue: image},
      --eval_set, {inputValue: eval_set},
      --task_name, {inputValue: task_name},
      --s3_model_dir, {inputValue: s3_model_dir},
      --s3_input_datadir, {inputValue: s3_input_datadir},
      --dataset_subfolder, {inputValue: dataset_subfolder},
      --s3_output_datadir, {outputPath: s3_output_datadir},
      --val_batch_size, {inputValue: val_batch_size},
      --, {inputValue: additional_args}
    ]
