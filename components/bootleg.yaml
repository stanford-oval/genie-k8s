name: Bootleg
description: |
  Run a Bootleg model to extract and dump candidate features
inputs:
  - {name: image, description: ''}
  - {name: s3_bucket, description: ''}
  - {name: owner, description: ''}
  - {name: task_name, description: ''}
  - {name: project, description: ''}
  - {name: experiment, description: ''}
  - {name: data_splits, description: ''}
  - {name: s3_datadir, description: ''}
  - {name: s3_database_dir, description: ''}
  - {name: s3_bootleg_subfolder, description: ''}
  - {name: dataset_subfolder, description: ''}
  - {name: train_languages, description: ''}
  - {name: eval_languages, description: ''}
  - {name: bootleg_model, description: ''}
  - {name: additional_args, description: ''}
outputs:
  - {name: s3_bootleg_prepped_data}
implementation:
  container:
    image: '{{inputs.parameters.image}}'
    command:
    - /bin/bash
    - -ex
    - -c
    - |
      . /opt/genie-toolkit/lib.sh

      parse_args "$0" "image s3_bucket owner task_name project experiment data_splits s3_datadir s3_database_dir s3_bootleg_subfolder dataset_subfolder train_languages eval_languages s3_bootleg_prepped_data bootleg_model" "$@"
      shift $n
      cd $HOME

      /opt/genie-toolkit/sync-repos.sh

      modeldir="$HOME/models/$bootleg_model"
      mkdir -p "$modeldir"
      mkdir -p cache

      keyctl session ; export AZCOPY_SPA_CLIENT_SECRET=${AZURE_SP_PASSWORD} ; azcopy login --service-principal --application-id ${AZURE_SP_APP_ID} --tenant-id ${AZURE_SP_TENANT_ID}

      mkdir -p ./database/
      azcopy sync --recursive=true ${s3_database_dir%/}/bootleg/ ./database/

      if test "${dataset_subfolder}" = "None" ; then
        dataset_subfolder="/"
      fi

      if test "${s3_bootleg_subfolder}" != "None" ; then
        s3_bootleg_subfolder=${s3_bootleg_subfolder}/
      else
        s3_bootleg_subfolder="/"
      fi

      case $task_name in
        "almond"*)
          dataset_dir=dataset/almond/
        ;;
        *)
           dataset_dir=dataset/
        ;;
      esac

      mkdir -p $dataset_dir
      s3_datadir = https://nfs009a5d03c43b4e7e8ec2.blob.core.windows.net/pvc-a8853620-9ac7-4885-a30e-0ec357f17bb6/${s3_datadir:13}
      azcopy sync --recursive=true ${s3_datadir%/}/${dataset_subfolder%/}/ $dataset_dir

      genienlp bootleg-dump-features \
          --data "dataset" \
          --save "$modeldir" \
          --tasks ${task_name} \
          --preserve_case \
          --exist_ok \
          --train_languages $train_languages \
          --eval_languages $eval_languages \
          --database_dir ./database/ \
          --bootleg_model ${bootleg_model} \
          --bootleg_data_splits ${data_splits} \
          --bootleg_output_dir results_temp \
          $@

      rm -fr "$modeldir/cache"
      rm -fr "$modeldir/dataset"

      S3_BOOTLEG_PREPPED_DATA=${s3_datadir%/}/${dataset_subfolder%/}/bootleg/${bootleg_model}/prepped_files/`date +%s`/${s3_bootleg_subfolder%/}/
      azcopy sync --recursive=true results_temp ${S3_BOOTLEG_PREPPED_DATA}

      mkdir -p `dirname $s3_bootleg_prepped_data`
      echo ${S3_BOOTLEG_PREPPED_DATA} > $s3_bootleg_prepped_data

    args: [
      'cmd',
      --image, {inputValue: image},
      --s3_bucket, {inputValue: s3_bucket},
      --owner, {inputValue: owner},
      --task_name, {inputValue: task_name},
      --project, {inputValue: project},
      --experiment, {inputValue: experiment},
      --data_splits, {inputValue: data_splits},
      --s3_datadir, {inputValue: s3_datadir},
      --s3_database_dir, {inputValue: s3_database_dir},
      --s3_bootleg_subfolder, {inputValue: s3_bootleg_subfolder},
      --dataset_subfolder, {inputValue: dataset_subfolder},
      --train_languages, {inputValue: train_languages},
      --eval_languages, {inputValue: eval_languages},
      --bootleg_model, {inputValue: bootleg_model},
      --s3_bootleg_prepped_data, { outputPath: s3_bootleg_prepped_data },
      --,
      {inputValue: additional_args},
    ]
